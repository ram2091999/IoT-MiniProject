{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Main_Model - Deep AE.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6sd48t_oHSy6"
      },
      "source": [
        "## SKIP TO THE LAST SECTION - Load and Run Link"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p0ay-txwrSOF",
        "outputId": "0ef3e221-22a1-440d-a42a-dc0d6357bfe3"
      },
      "source": [
        "!git clone https://github.com/sergts/botnet-traffic-analysis.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'botnet-traffic-analysis'...\n",
            "remote: Enumerating objects: 238, done.\u001b[K\n",
            "remote: Total 238 (delta 0), reused 0 (delta 0), pack-reused 238\u001b[K\n",
            "Receiving objects: 100% (238/238), 6.90 MiB | 11.16 MiB/s, done.\n",
            "Resolving deltas: 100% (143/143), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Sn2vpf0rz4Y",
        "outputId": "687824c4-18b1-456a-d37a-598fdf990a06"
      },
      "source": [
        "!pip install wget"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting wget\n",
            "  Downloading https://files.pythonhosted.org/packages/47/6a/62e288da7bcda82b935ff0c6cfe542970f04e29c756b0e147251b2fb251f/wget-3.2.zip\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-cp37-none-any.whl size=9681 sha256=0ca8dcd7083c2a0bf0993f27a2c49761c1356685f26dffe63fc8fac199bec6eb\n",
            "  Stored in directory: /root/.cache/pip/wheels/40/15/30/7d8f7cea2902b4db79e3fea550d7d7b85ecb27ef992b618f3f\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_UEl9AhkrfTG",
        "outputId": "15579592-2c91-46ef-f1e2-05e9a7d5e79c"
      },
      "source": [
        "import os\n",
        "os.chdir('/content/botnet-traffic-analysis')\n",
        "!python3 /content/botnet-traffic-analysis/download_data.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "https://archive.ics.uci.edu/ml/machine-learning-databases/00442/Danmini_Doorbell/benign_traffic.csv\n",
            "https://archive.ics.uci.edu/ml/machine-learning-databases/00442/Danmini_Doorbell/gafgyt_attacks.rar\n",
            "https://archive.ics.uci.edu/ml/machine-learning-databases/00442/Danmini_Doorbell/mirai_attacks.rar\n",
            "https://archive.ics.uci.edu/ml/machine-learning-databases/00442/Ecobee_Thermostat/benign_traffic.csv\n",
            "https://archive.ics.uci.edu/ml/machine-learning-databases/00442/Ecobee_Thermostat/gafgyt_attacks.rar\n",
            "https://archive.ics.uci.edu/ml/machine-learning-databases/00442/Ecobee_Thermostat/mirai_attacks.rar\n",
            "https://archive.ics.uci.edu/ml/machine-learning-databases/00442/Ennio_Doorbell/benign_traffic.csv\n",
            "https://archive.ics.uci.edu/ml/machine-learning-databases/00442/Ennio_Doorbell/gafgyt_attacks.rar\n",
            "https://archive.ics.uci.edu/ml/machine-learning-databases/00442/Philips_B120N10_Baby_Monitor/benign_traffic.csv\n",
            "https://archive.ics.uci.edu/ml/machine-learning-databases/00442/Philips_B120N10_Baby_Monitor/gafgyt_attacks.rar\n",
            "https://archive.ics.uci.edu/ml/machine-learning-databases/00442/Philips_B120N10_Baby_Monitor/mirai_attacks.rar\n",
            "https://archive.ics.uci.edu/ml/machine-learning-databases/00442/Provision_PT_737E_Security_Camera/benign_traffic.csv\n",
            "https://archive.ics.uci.edu/ml/machine-learning-databases/00442/Provision_PT_737E_Security_Camera/gafgyt_attacks.rar\n",
            "https://archive.ics.uci.edu/ml/machine-learning-databases/00442/Provision_PT_737E_Security_Camera/mirai_attacks.rar\n",
            "https://archive.ics.uci.edu/ml/machine-learning-databases/00442/Provision_PT_838_Security_Camera/benign_traffic.csv\n",
            "https://archive.ics.uci.edu/ml/machine-learning-databases/00442/Provision_PT_838_Security_Camera/gafgyt_attacks.rar\n",
            "https://archive.ics.uci.edu/ml/machine-learning-databases/00442/Provision_PT_838_Security_Camera/mirai_attacks.rar\n",
            "https://archive.ics.uci.edu/ml/machine-learning-databases/00442/Samsung_SNH_1011_N_Webcam/benign_traffic.csv\n",
            "https://archive.ics.uci.edu/ml/machine-learning-databases/00442/Samsung_SNH_1011_N_Webcam/gafgyt_attacks.rar\n",
            "https://archive.ics.uci.edu/ml/machine-learning-databases/00442/SimpleHome_XCS7_1002_WHT_Security_Camera/benign_traffic.csv\n",
            "https://archive.ics.uci.edu/ml/machine-learning-databases/00442/SimpleHome_XCS7_1002_WHT_Security_Camera/gafgyt_attacks.rar\n",
            "https://archive.ics.uci.edu/ml/machine-learning-databases/00442/SimpleHome_XCS7_1002_WHT_Security_Camera/mirai_attacks.rar\n",
            "https://archive.ics.uci.edu/ml/machine-learning-databases/00442/SimpleHome_XCS7_1003_WHT_Security_Camera/benign_traffic.csv\n",
            "https://archive.ics.uci.edu/ml/machine-learning-databases/00442/SimpleHome_XCS7_1003_WHT_Security_Camera/gafgyt_attacks.rar\n",
            "https://archive.ics.uci.edu/ml/machine-learning-databases/00442/SimpleHome_XCS7_1003_WHT_Security_Camera/mirai_attacks.rar\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fAHW417irkD9",
        "outputId": "b64a39d1-83cc-4a3e-9b2e-95f4a824be04"
      },
      "source": [
        "import sys\n",
        "sys.path.append('/content/botnet-traffic-analysis/anomaly-detection/')\n",
        "!python3 /content/botnet-traffic-analysis/anomaly-detection/train.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-04-05 15:45:54.124263: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "Loading combined training data...\n",
            "2021-04-05 15:46:05.703142: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
            "2021-04-05 15:46:05.722309: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
            "2021-04-05 15:46:05.771996: E tensorflow/stream_executor/cuda/cuda_driver.cc:328] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "2021-04-05 15:46:05.772052: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (8201cda12089): /proc/driver/nvidia/version does not exist\n",
            "2021-04-05 15:46:05.772452: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX512F\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-04-05 15:46:05.772631: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
            "2021-04-05 15:46:05.900194: I tensorflow/core/profiler/lib/profiler_session.cc:136] Profiler session initializing.\n",
            "2021-04-05 15:46:05.900253: I tensorflow/core/profiler/lib/profiler_session.cc:155] Profiler session started.\n",
            "2021-04-05 15:46:05.910786: I tensorflow/core/profiler/lib/profiler_session.cc:172] Profiler session tear down.\n",
            "Training model for all data combined\n",
            "2021-04-05 15:46:05.989068: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
            "2021-04-05 15:46:06.001048: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 1999995000 Hz\n",
            "Epoch 1/500\n",
            "   1/2896 [..............................] - ETA: 24:35 - loss: 0.61402021-04-05 15:46:06.575998: I tensorflow/core/profiler/lib/profiler_session.cc:136] Profiler session initializing.\n",
            "2021-04-05 15:46:06.576058: I tensorflow/core/profiler/lib/profiler_session.cc:155] Profiler session started.\n",
            "2021-04-05 15:46:06.580650: I tensorflow/core/profiler/lib/profiler_session.cc:71] Profiler session collecting data.\n",
            "2021-04-05 15:46:06.583056: I tensorflow/core/profiler/lib/profiler_session.cc:172] Profiler session tear down.\n",
            "2021-04-05 15:46:06.588077: I tensorflow/core/profiler/rpc/client/save_profile.cc:137] Creating directory: ./logs/train/plugins/profile/2021_04_05_15_46_06\n",
            "2021-04-05 15:46:06.589016: I tensorflow/core/profiler/rpc/client/save_profile.cc:143] Dumped gzipped tool data for trace.json.gz to ./logs/train/plugins/profile/2021_04_05_15_46_06/8201cda12089.trace.json.gz\n",
            "2021-04-05 15:46:06.593676: I tensorflow/core/profiler/rpc/client/save_profile.cc:137] Creating directory: ./logs/train/plugins/profile/2021_04_05_15_46_06\n",
            "2021-04-05 15:46:06.593856: I tensorflow/core/profiler/rpc/client/save_profile.cc:143] Dumped gzipped tool data for memory_profile.json.gz to ./logs/train/plugins/profile/2021_04_05_15_46_06/8201cda12089.memory_profile.json.gz\n",
            "2021-04-05 15:46:06.594266: I tensorflow/core/profiler/rpc/client/capture_profile.cc:251] Creating directory: ./logs/train/plugins/profile/2021_04_05_15_46_06Dumped tool data for xplane.pb to ./logs/train/plugins/profile/2021_04_05_15_46_06/8201cda12089.xplane.pb\n",
            "Dumped tool data for overview_page.pb to ./logs/train/plugins/profile/2021_04_05_15_46_06/8201cda12089.overview_page.pb\n",
            "Dumped tool data for input_pipeline.pb to ./logs/train/plugins/profile/2021_04_05_15_46_06/8201cda12089.input_pipeline.pb\n",
            "Dumped tool data for tensorflow_stats.pb to ./logs/train/plugins/profile/2021_04_05_15_46_06/8201cda12089.tensorflow_stats.pb\n",
            "Dumped tool data for kernel_stats.pb to ./logs/train/plugins/profile/2021_04_05_15_46_06/8201cda12089.kernel_stats.pb\n",
            "\n",
            "2896/2896 [==============================] - 6s 2ms/step - loss: 0.6670 - val_loss: 0.4493\n",
            "Epoch 2/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.4279 - val_loss: 0.3173\n",
            "Epoch 3/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.3069 - val_loss: 0.2825\n",
            "Epoch 4/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.2900 - val_loss: 0.2546\n",
            "Epoch 5/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.2491 - val_loss: 0.2326\n",
            "Epoch 6/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.2268 - val_loss: 0.2242\n",
            "Epoch 7/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.2168 - val_loss: 0.2050\n",
            "Epoch 8/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.2056 - val_loss: 0.1834\n",
            "Epoch 9/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.1836 - val_loss: 0.1674\n",
            "Epoch 10/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.1676 - val_loss: 0.1564\n",
            "Epoch 11/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.1575 - val_loss: 0.1478\n",
            "Epoch 12/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.1481 - val_loss: 0.1432\n",
            "Epoch 13/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.1374 - val_loss: 0.1358\n",
            "Epoch 14/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.1418 - val_loss: 0.1339\n",
            "Epoch 15/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.1385 - val_loss: 0.1265\n",
            "Epoch 16/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.1239 - val_loss: 0.1271\n",
            "Epoch 17/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.1263 - val_loss: 0.1415\n",
            "Epoch 18/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.1219 - val_loss: 0.1171\n",
            "Epoch 19/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.1177 - val_loss: 0.1241\n",
            "Epoch 20/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.1157 - val_loss: 0.1115\n",
            "Epoch 21/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.1139 - val_loss: 0.1095\n",
            "Epoch 22/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.1135 - val_loss: 0.1073\n",
            "Epoch 23/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.1064 - val_loss: 0.1067\n",
            "Epoch 24/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.1054 - val_loss: 0.1064\n",
            "Epoch 25/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.1061 - val_loss: 0.1018\n",
            "Epoch 26/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.1003 - val_loss: 0.1003\n",
            "Epoch 27/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0996 - val_loss: 0.0984\n",
            "Epoch 28/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0982 - val_loss: 0.0978\n",
            "Epoch 29/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0963 - val_loss: 0.0959\n",
            "Epoch 30/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0941 - val_loss: 0.0973\n",
            "Epoch 31/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.1036 - val_loss: 0.0934\n",
            "Epoch 32/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0951 - val_loss: 0.0917\n",
            "Epoch 33/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0933 - val_loss: 0.0908\n",
            "Epoch 34/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0899 - val_loss: 0.0896\n",
            "Epoch 35/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0884 - val_loss: 0.0913\n",
            "Epoch 36/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0873 - val_loss: 0.0866\n",
            "Epoch 37/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0873 - val_loss: 0.0901\n",
            "Epoch 38/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0871 - val_loss: 0.0845\n",
            "Epoch 39/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0845 - val_loss: 0.0842\n",
            "Epoch 40/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0816 - val_loss: 0.0829\n",
            "Epoch 41/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0829 - val_loss: 0.0817\n",
            "Epoch 42/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0825 - val_loss: 0.0803\n",
            "Epoch 43/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0821 - val_loss: 0.0791\n",
            "Epoch 44/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0780 - val_loss: 0.0787\n",
            "Epoch 45/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0779 - val_loss: 0.0805\n",
            "Epoch 46/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0777 - val_loss: 0.0783\n",
            "Epoch 47/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0760 - val_loss: 0.0777\n",
            "Epoch 48/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0753 - val_loss: 0.0746\n",
            "Epoch 49/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0735 - val_loss: 0.0742\n",
            "Epoch 50/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0751 - val_loss: 0.0740\n",
            "Epoch 51/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0738 - val_loss: 0.0755\n",
            "Epoch 52/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0735 - val_loss: 0.0726\n",
            "Epoch 53/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0723 - val_loss: 0.0714\n",
            "Epoch 54/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0717 - val_loss: 0.0707\n",
            "Epoch 55/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0713 - val_loss: 0.0697\n",
            "Epoch 56/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0711 - val_loss: 0.0682\n",
            "Epoch 57/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0703 - val_loss: 0.0681\n",
            "Epoch 58/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0675 - val_loss: 0.0698\n",
            "Epoch 59/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0678 - val_loss: 0.0668\n",
            "Epoch 60/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0669 - val_loss: 0.0658\n",
            "Epoch 61/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0662 - val_loss: 0.1397\n",
            "Epoch 62/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0657 - val_loss: 0.0665\n",
            "Epoch 63/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0640 - val_loss: 0.0647\n",
            "Epoch 64/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0634 - val_loss: 0.0651\n",
            "Epoch 65/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0648 - val_loss: 0.0624\n",
            "Epoch 66/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0634 - val_loss: 0.0621\n",
            "Epoch 67/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0635 - val_loss: 0.0628\n",
            "Epoch 68/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0630 - val_loss: 0.0787\n",
            "Epoch 69/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0626 - val_loss: 0.0609\n",
            "Epoch 70/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0623 - val_loss: 0.0605\n",
            "Epoch 71/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0597 - val_loss: 0.0608\n",
            "Epoch 72/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0591 - val_loss: 0.1134\n",
            "Epoch 73/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0599 - val_loss: 0.0582\n",
            "Epoch 74/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0622 - val_loss: 0.0585\n",
            "Epoch 75/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0574 - val_loss: 0.0573\n",
            "Epoch 76/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0574 - val_loss: 0.0567\n",
            "Epoch 77/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0563 - val_loss: 0.0610\n",
            "Epoch 78/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0556 - val_loss: 0.0565\n",
            "Epoch 79/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0550 - val_loss: 0.0551\n",
            "Epoch 80/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0552 - val_loss: 0.0550\n",
            "Epoch 81/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0549 - val_loss: 0.0538\n",
            "Epoch 82/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0544 - val_loss: 0.0533\n",
            "Epoch 83/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0550 - val_loss: 0.0601\n",
            "Epoch 84/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0529 - val_loss: 0.0623\n",
            "Epoch 85/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0532 - val_loss: 0.0569\n",
            "Epoch 86/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0546 - val_loss: 0.0545\n",
            "Epoch 87/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0514 - val_loss: 0.0515\n",
            "Epoch 88/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0511 - val_loss: 0.0521\n",
            "Epoch 89/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0509 - val_loss: 0.0540\n",
            "Epoch 90/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0506 - val_loss: 0.0509\n",
            "Epoch 91/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0502 - val_loss: 0.0508\n",
            "Epoch 92/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0490 - val_loss: 0.0500\n",
            "Epoch 93/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0490 - val_loss: 0.0525\n",
            "Epoch 94/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0498 - val_loss: 0.0508\n",
            "Epoch 95/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0507 - val_loss: 0.0504\n",
            "Epoch 96/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0478 - val_loss: 0.0590\n",
            "Epoch 97/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0489 - val_loss: 0.0501\n",
            "Epoch 98/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0487 - val_loss: 0.0540\n",
            "Epoch 99/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0470 - val_loss: 0.0501\n",
            "Epoch 100/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0471 - val_loss: 0.0476\n",
            "Epoch 101/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0473 - val_loss: 0.0473\n",
            "Epoch 102/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0451 - val_loss: 0.0484\n",
            "Epoch 103/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0470 - val_loss: 0.0504\n",
            "Epoch 104/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0466 - val_loss: 0.0468\n",
            "Epoch 105/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0471 - val_loss: 0.0472\n",
            "Epoch 106/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0467 - val_loss: 0.0469\n",
            "Epoch 107/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0467 - val_loss: 0.0467\n",
            "Epoch 108/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0458 - val_loss: 0.0458\n",
            "Epoch 109/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0437 - val_loss: 0.0468\n",
            "Epoch 110/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0466 - val_loss: 0.0456\n",
            "Epoch 111/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0452 - val_loss: 0.0451\n",
            "Epoch 112/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0439 - val_loss: 0.0456\n",
            "Epoch 113/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0446 - val_loss: 0.0459\n",
            "Epoch 114/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0446 - val_loss: 0.0449\n",
            "Epoch 115/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0446 - val_loss: 0.0447\n",
            "Epoch 116/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0432 - val_loss: 0.0450\n",
            "Epoch 117/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0455 - val_loss: 0.0447\n",
            "Epoch 118/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0436 - val_loss: 0.0457\n",
            "Epoch 119/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0436 - val_loss: 0.0445\n",
            "Epoch 120/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0437 - val_loss: 0.0450\n",
            "Epoch 121/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0451 - val_loss: 0.0449\n",
            "Epoch 122/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0424 - val_loss: 0.0440\n",
            "Epoch 123/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0426 - val_loss: 0.0434\n",
            "Epoch 124/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0430 - val_loss: 0.0437\n",
            "Epoch 125/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0431 - val_loss: 0.0448\n",
            "Epoch 126/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0437 - val_loss: 0.0469\n",
            "Epoch 127/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0433 - val_loss: 0.0430\n",
            "Epoch 128/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0420 - val_loss: 0.0441\n",
            "Epoch 129/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0435 - val_loss: 0.0433\n",
            "Epoch 130/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0427 - val_loss: 0.0432\n",
            "Epoch 131/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0417 - val_loss: 0.0425\n",
            "Epoch 132/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0430 - val_loss: 0.0428\n",
            "Epoch 133/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0422 - val_loss: 0.0424\n",
            "Epoch 134/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0418 - val_loss: 0.0425\n",
            "Epoch 135/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0412 - val_loss: 0.0423\n",
            "Epoch 136/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0411 - val_loss: 0.0426\n",
            "Epoch 137/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0421 - val_loss: 0.0420\n",
            "Epoch 138/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0413 - val_loss: 0.0424\n",
            "Epoch 139/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0403 - val_loss: 0.0423\n",
            "Epoch 140/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0404 - val_loss: 0.0415\n",
            "Epoch 141/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0408 - val_loss: 0.0533\n",
            "Epoch 142/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0416 - val_loss: 0.0416\n",
            "Epoch 143/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0410 - val_loss: 0.0421\n",
            "Epoch 144/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0413 - val_loss: 0.0410\n",
            "Epoch 145/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0408 - val_loss: 0.0421\n",
            "Epoch 146/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0405 - val_loss: 0.0413\n",
            "Epoch 147/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0405 - val_loss: 0.0406\n",
            "Epoch 148/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0394 - val_loss: 0.0424\n",
            "Epoch 149/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0395 - val_loss: 0.0410\n",
            "Epoch 150/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0409 - val_loss: 0.0404\n",
            "Epoch 151/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0399 - val_loss: 0.0405\n",
            "Epoch 152/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0398 - val_loss: 0.0447\n",
            "Epoch 153/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0397 - val_loss: 0.0405\n",
            "Epoch 154/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0388 - val_loss: 0.0475\n",
            "Epoch 155/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0399 - val_loss: 0.0404\n",
            "Epoch 156/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0406 - val_loss: 0.0401\n",
            "Epoch 157/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0397 - val_loss: 0.0405\n",
            "Epoch 158/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0385 - val_loss: 0.0394\n",
            "Epoch 159/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0392 - val_loss: 0.0408\n",
            "Epoch 160/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0401 - val_loss: 0.0393\n",
            "Epoch 161/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0387 - val_loss: 0.0399\n",
            "Epoch 162/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0393 - val_loss: 0.0401\n",
            "Epoch 163/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0389 - val_loss: 0.0397\n",
            "Epoch 164/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0383 - val_loss: 0.0394\n",
            "Epoch 165/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0382 - val_loss: 0.0397\n",
            "Epoch 166/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0391 - val_loss: 0.0389\n",
            "Epoch 167/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0383 - val_loss: 0.0399\n",
            "Epoch 168/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0385 - val_loss: 0.0389\n",
            "Epoch 169/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0382 - val_loss: 0.0394\n",
            "Epoch 170/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0381 - val_loss: 0.0429\n",
            "Epoch 171/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0379 - val_loss: 0.0393\n",
            "Epoch 172/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0379 - val_loss: 0.0418\n",
            "Epoch 173/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0372 - val_loss: 0.0390\n",
            "Epoch 174/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0376 - val_loss: 0.0387\n",
            "Epoch 175/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0382 - val_loss: 0.0395\n",
            "Epoch 176/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0381 - val_loss: 0.0381\n",
            "Epoch 177/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0376 - val_loss: 0.0391\n",
            "Epoch 178/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0372 - val_loss: 0.0434\n",
            "Epoch 179/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0373 - val_loss: 0.0381\n",
            "Epoch 180/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0382 - val_loss: 0.0380\n",
            "Epoch 181/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0376 - val_loss: 0.0382\n",
            "Epoch 182/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0378 - val_loss: 0.0402\n",
            "Epoch 183/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0381 - val_loss: 0.0372\n",
            "Epoch 184/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0376 - val_loss: 0.0417\n",
            "Epoch 185/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0373 - val_loss: 0.0405\n",
            "Epoch 186/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0376 - val_loss: 0.0378\n",
            "Epoch 187/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0371 - val_loss: 0.0411\n",
            "Epoch 188/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0371 - val_loss: 0.0379\n",
            "Epoch 189/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0371 - val_loss: 0.0388\n",
            "Epoch 190/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0378 - val_loss: 0.0370\n",
            "Epoch 191/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0369 - val_loss: 0.0387\n",
            "Epoch 192/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0369 - val_loss: 0.0372\n",
            "Epoch 193/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0372 - val_loss: 0.0381\n",
            "Epoch 194/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0361 - val_loss: 0.0390\n",
            "Epoch 195/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0366 - val_loss: 0.0369\n",
            "Epoch 196/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0367 - val_loss: 0.0371\n",
            "Epoch 197/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0371 - val_loss: 0.0377\n",
            "Epoch 198/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0375 - val_loss: 0.0370\n",
            "Epoch 199/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0356 - val_loss: 0.0373\n",
            "Epoch 200/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0355 - val_loss: 0.0402\n",
            "Epoch 201/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0365 - val_loss: 0.0378\n",
            "Epoch 202/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0371 - val_loss: 0.0379\n",
            "Epoch 203/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0353 - val_loss: 0.0373\n",
            "Epoch 204/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0362 - val_loss: 0.0367\n",
            "Epoch 205/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0359 - val_loss: 0.0364\n",
            "Epoch 206/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0354 - val_loss: 0.0399\n",
            "Epoch 207/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0355 - val_loss: 0.0360\n",
            "Epoch 208/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0357 - val_loss: 0.0367\n",
            "Epoch 209/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0356 - val_loss: 0.0373\n",
            "Epoch 210/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0359 - val_loss: 0.0361\n",
            "Epoch 211/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0352 - val_loss: 0.0362\n",
            "Epoch 212/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0352 - val_loss: 0.0358\n",
            "Epoch 213/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0362 - val_loss: 0.0375\n",
            "Epoch 214/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0350 - val_loss: 0.0365\n",
            "Epoch 215/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0350 - val_loss: 0.0362\n",
            "Epoch 216/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0342 - val_loss: 0.0358\n",
            "Epoch 217/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0353 - val_loss: 0.0362\n",
            "Epoch 218/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0345 - val_loss: 0.0361\n",
            "Epoch 219/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0355 - val_loss: 0.0375\n",
            "Epoch 220/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0353 - val_loss: 0.0356\n",
            "Epoch 221/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0351 - val_loss: 0.0355\n",
            "Epoch 222/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0374 - val_loss: 0.0363\n",
            "Epoch 223/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0348 - val_loss: 0.0357\n",
            "Epoch 224/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0358 - val_loss: 0.0355\n",
            "Epoch 225/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0355 - val_loss: 0.0352\n",
            "Epoch 226/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0346 - val_loss: 0.0356\n",
            "Epoch 227/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0357 - val_loss: 0.0356\n",
            "Epoch 228/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0354 - val_loss: 0.0360\n",
            "Epoch 229/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0344 - val_loss: 0.0348\n",
            "Epoch 230/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0337 - val_loss: 0.0346\n",
            "Epoch 231/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0350 - val_loss: 0.0344\n",
            "Epoch 232/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0350 - val_loss: 0.0385\n",
            "Epoch 233/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0348 - val_loss: 0.0347\n",
            "Epoch 234/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0333 - val_loss: 0.0345\n",
            "Epoch 235/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0337 - val_loss: 0.0364\n",
            "Epoch 236/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0332 - val_loss: 0.0391\n",
            "Epoch 237/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0349 - val_loss: 0.0340\n",
            "Epoch 238/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0339 - val_loss: 0.0344\n",
            "Epoch 239/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0345 - val_loss: 0.0344\n",
            "Epoch 240/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0337 - val_loss: 0.0341\n",
            "Epoch 241/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0330 - val_loss: 0.0346\n",
            "Epoch 242/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0339 - val_loss: 0.0342\n",
            "Epoch 243/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0331 - val_loss: 0.0341\n",
            "Epoch 244/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0344 - val_loss: 0.0338\n",
            "Epoch 245/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0329 - val_loss: 0.0337\n",
            "Epoch 246/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0328 - val_loss: 0.0342\n",
            "Epoch 247/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0340 - val_loss: 0.0393\n",
            "Epoch 248/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0341 - val_loss: 0.0341\n",
            "Epoch 249/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0331 - val_loss: 0.0344\n",
            "Epoch 250/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0324 - val_loss: 0.0338\n",
            "Epoch 251/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0328 - val_loss: 0.0342\n",
            "Epoch 252/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0327 - val_loss: 0.0332\n",
            "Epoch 253/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0331 - val_loss: 0.0335\n",
            "Epoch 254/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0358 - val_loss: 0.0356\n",
            "Epoch 255/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0332 - val_loss: 0.0333\n",
            "Epoch 256/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0332 - val_loss: 0.0328\n",
            "Epoch 257/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0327 - val_loss: 0.0337\n",
            "Epoch 258/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0328 - val_loss: 0.0329\n",
            "Epoch 259/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0320 - val_loss: 0.0331\n",
            "Epoch 260/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0347 - val_loss: 0.0334\n",
            "Epoch 261/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0326 - val_loss: 0.0344\n",
            "Epoch 262/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0326 - val_loss: 0.0361\n",
            "Epoch 263/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0322 - val_loss: 0.0330\n",
            "Epoch 264/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0321 - val_loss: 0.0337\n",
            "Epoch 265/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0322 - val_loss: 0.0325\n",
            "Epoch 266/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0322 - val_loss: 0.0327\n",
            "Epoch 267/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0325 - val_loss: 0.0327\n",
            "Epoch 268/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0322 - val_loss: 0.0321\n",
            "Epoch 269/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0320 - val_loss: 0.0395\n",
            "Epoch 270/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0317 - val_loss: 0.0322\n",
            "Epoch 271/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0313 - val_loss: 0.0319\n",
            "Epoch 272/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0314 - val_loss: 0.0320\n",
            "Epoch 273/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0312 - val_loss: 0.0320\n",
            "Epoch 274/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0318 - val_loss: 0.0319\n",
            "Epoch 275/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0313 - val_loss: 0.0317\n",
            "Epoch 276/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0318 - val_loss: 0.0317\n",
            "Epoch 277/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0315 - val_loss: 0.0365\n",
            "Epoch 278/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0318 - val_loss: 0.0318\n",
            "Epoch 279/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0315 - val_loss: 0.0318\n",
            "Epoch 280/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0315 - val_loss: 0.0328\n",
            "Epoch 281/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0312 - val_loss: 0.0355\n",
            "Epoch 282/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0313 - val_loss: 0.0315\n",
            "Epoch 283/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0314 - val_loss: 0.0317\n",
            "Epoch 284/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0310 - val_loss: 0.0318\n",
            "Epoch 285/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0317 - val_loss: 0.0316\n",
            "Epoch 286/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0319 - val_loss: 0.0326\n",
            "Epoch 287/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0311 - val_loss: 0.0317\n",
            "Epoch 288/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0309 - val_loss: 0.0316\n",
            "Epoch 289/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0472 - val_loss: 0.0327\n",
            "Epoch 290/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0314 - val_loss: 0.0315\n",
            "Epoch 291/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0304 - val_loss: 0.0315\n",
            "Epoch 292/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0307 - val_loss: 0.0314\n",
            "Epoch 293/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0313 - val_loss: 0.0318\n",
            "Epoch 294/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0308 - val_loss: 0.0318\n",
            "Epoch 295/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0344 - val_loss: 0.0314\n",
            "Epoch 296/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0307 - val_loss: 0.0325\n",
            "Epoch 297/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0307 - val_loss: 0.0352\n",
            "Epoch 298/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0301 - val_loss: 0.0317\n",
            "Epoch 299/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0310 - val_loss: 0.0312\n",
            "Epoch 300/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0311 - val_loss: 0.0312\n",
            "Epoch 301/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0309 - val_loss: 0.0313\n",
            "Epoch 302/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0411 - val_loss: 0.0425\n",
            "Epoch 303/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0316 - val_loss: 0.0315\n",
            "Epoch 304/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0302 - val_loss: 0.0313\n",
            "Epoch 305/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0309 - val_loss: 0.0315\n",
            "Epoch 306/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0305 - val_loss: 0.0314\n",
            "Epoch 307/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0307 - val_loss: 0.0310\n",
            "Epoch 308/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0320 - val_loss: 0.0310\n",
            "Epoch 309/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0304 - val_loss: 0.0309\n",
            "Epoch 310/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0306 - val_loss: 0.0310\n",
            "Epoch 311/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0310 - val_loss: 0.0316\n",
            "Epoch 312/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0306 - val_loss: 0.0308\n",
            "Epoch 313/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0305 - val_loss: 0.0313\n",
            "Epoch 314/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0301 - val_loss: 0.0309\n",
            "Epoch 315/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0306 - val_loss: 0.0318\n",
            "Epoch 316/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0305 - val_loss: 0.0330\n",
            "Epoch 317/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0313 - val_loss: 0.0307\n",
            "Epoch 318/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0303 - val_loss: 0.0312\n",
            "Epoch 319/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0302 - val_loss: 0.0313\n",
            "Epoch 320/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0301 - val_loss: 0.0317\n",
            "Epoch 321/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0308 - val_loss: 0.0313\n",
            "Epoch 322/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0310 - val_loss: 0.0309\n",
            "Epoch 323/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0302 - val_loss: 0.0306\n",
            "Epoch 324/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0311 - val_loss: 0.0316\n",
            "Epoch 325/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0310 - val_loss: 0.0308\n",
            "Epoch 326/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0303 - val_loss: 0.0333\n",
            "Epoch 327/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0305 - val_loss: 0.0317\n",
            "Epoch 328/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0298 - val_loss: 0.0306\n",
            "Epoch 329/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0303 - val_loss: 0.0307\n",
            "Epoch 330/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0356 - val_loss: 0.0310\n",
            "Epoch 331/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0305 - val_loss: 0.0308\n",
            "Epoch 332/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0301 - val_loss: 0.0308\n",
            "Epoch 333/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0308 - val_loss: 0.0307\n",
            "Epoch 334/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0298 - val_loss: 0.0313\n",
            "Epoch 335/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0308 - val_loss: 0.0306\n",
            "Epoch 336/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0319 - val_loss: 0.0314\n",
            "Epoch 337/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0307 - val_loss: 0.0315\n",
            "Epoch 338/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0301 - val_loss: 0.0307\n",
            "Epoch 339/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0311 - val_loss: 0.0303\n",
            "Epoch 340/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0297 - val_loss: 0.0322\n",
            "Epoch 341/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0305 - val_loss: 0.0309\n",
            "Epoch 342/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0306 - val_loss: 0.0303\n",
            "Epoch 343/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0302 - val_loss: 0.0308\n",
            "Epoch 344/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0311 - val_loss: 0.0319\n",
            "Epoch 345/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0311 - val_loss: 0.0306\n",
            "Epoch 346/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0301 - val_loss: 0.0305\n",
            "Epoch 347/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0297 - val_loss: 0.0306\n",
            "Epoch 348/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0303 - val_loss: 0.0304\n",
            "Epoch 349/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0303 - val_loss: 0.0323\n",
            "Epoch 350/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0301 - val_loss: 0.0335\n",
            "Epoch 351/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0302 - val_loss: 0.0312\n",
            "Epoch 352/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0309 - val_loss: 0.0304\n",
            "Epoch 353/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0307 - val_loss: 0.0311\n",
            "Epoch 354/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0301 - val_loss: 0.0306\n",
            "Epoch 355/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0304 - val_loss: 0.0300\n",
            "Epoch 356/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0304 - val_loss: 0.0304\n",
            "Epoch 357/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0304 - val_loss: 0.0302\n",
            "Epoch 358/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0303 - val_loss: 0.0399\n",
            "Epoch 359/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0301 - val_loss: 0.0301\n",
            "Epoch 360/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0299 - val_loss: 0.0305\n",
            "Epoch 361/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0302 - val_loss: 0.0300\n",
            "Epoch 362/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0299 - val_loss: 0.0302\n",
            "Epoch 363/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0299 - val_loss: 0.0303\n",
            "Epoch 364/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0317 - val_loss: 0.0307\n",
            "Epoch 365/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0304 - val_loss: 0.0317\n",
            "Epoch 366/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0306 - val_loss: 0.0301\n",
            "Epoch 367/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0295 - val_loss: 0.0300\n",
            "Epoch 368/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0306 - val_loss: 0.0300\n",
            "Epoch 369/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0301 - val_loss: 0.0309\n",
            "Epoch 370/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0297 - val_loss: 0.0316\n",
            "Epoch 371/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0304 - val_loss: 0.0298\n",
            "Epoch 372/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0308 - val_loss: 0.0315\n",
            "Epoch 373/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0300 - val_loss: 0.0301\n",
            "Epoch 374/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0299 - val_loss: 0.0301\n",
            "Epoch 375/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0298 - val_loss: 0.0298\n",
            "Epoch 376/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0300 - val_loss: 0.0303\n",
            "Epoch 377/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0312 - val_loss: 0.0306\n",
            "Epoch 378/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0298 - val_loss: 0.0314\n",
            "Epoch 379/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0303 - val_loss: 0.0303\n",
            "Epoch 380/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0308 - val_loss: 0.0316\n",
            "Epoch 381/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0308 - val_loss: 0.0302\n",
            "Epoch 382/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0303 - val_loss: 0.0300\n",
            "Epoch 383/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0292 - val_loss: 0.0305\n",
            "Epoch 384/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0297 - val_loss: 0.0301\n",
            "Epoch 385/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0301 - val_loss: 0.0303\n",
            "Epoch 386/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0333 - val_loss: 0.0629\n",
            "Epoch 387/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0419 - val_loss: 0.0315\n",
            "Epoch 388/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0304 - val_loss: 0.0305\n",
            "Epoch 389/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0297 - val_loss: 0.0302\n",
            "Epoch 390/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0301 - val_loss: 0.0305\n",
            "Epoch 391/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0319 - val_loss: 0.0303\n",
            "Epoch 392/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0300 - val_loss: 0.0444\n",
            "Epoch 393/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0305 - val_loss: 0.0312\n",
            "Epoch 394/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0294 - val_loss: 0.5440\n",
            "Epoch 395/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0380 - val_loss: 0.0309\n",
            "Epoch 396/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0297 - val_loss: 0.0323\n",
            "Epoch 397/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0297 - val_loss: 0.0307\n",
            "Epoch 398/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0299 - val_loss: 0.0298\n",
            "Epoch 399/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0295 - val_loss: 0.0306\n",
            "Epoch 400/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.1682 - val_loss: 0.0354\n",
            "Epoch 401/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0310 - val_loss: 0.0308\n",
            "Epoch 402/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0301 - val_loss: 0.0307\n",
            "Epoch 403/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0314 - val_loss: 0.0319\n",
            "Epoch 404/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0301 - val_loss: 0.0298\n",
            "Epoch 405/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0301 - val_loss: 0.0310\n",
            "Epoch 406/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0298 - val_loss: 0.0307\n",
            "Epoch 407/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0292 - val_loss: 0.0302\n",
            "Epoch 408/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0309 - val_loss: 0.0321\n",
            "Epoch 409/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0299 - val_loss: 0.0311\n",
            "Epoch 410/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0295 - val_loss: 0.0304\n",
            "Epoch 411/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0292 - val_loss: 0.0293\n",
            "Epoch 412/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0293 - val_loss: 0.0292\n",
            "Epoch 413/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0336 - val_loss: 0.0325\n",
            "Epoch 414/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0305 - val_loss: 0.0296\n",
            "Epoch 415/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0293 - val_loss: 0.0308\n",
            "Epoch 416/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0298 - val_loss: 0.0291\n",
            "Epoch 417/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0289 - val_loss: 0.0298\n",
            "Epoch 418/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0298 - val_loss: 0.0295\n",
            "Epoch 419/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0287 - val_loss: 0.0334\n",
            "Epoch 420/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0298 - val_loss: 0.0297\n",
            "Epoch 421/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0314 - val_loss: 0.0299\n",
            "Epoch 422/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0281 - val_loss: 0.0304\n",
            "Epoch 423/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0295 - val_loss: 0.0302\n",
            "Epoch 424/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0291 - val_loss: 0.0290\n",
            "Epoch 425/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0290 - val_loss: 0.0293\n",
            "Epoch 426/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0292 - val_loss: 0.0307\n",
            "Epoch 427/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0293 - val_loss: 0.0289\n",
            "Epoch 428/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0288 - val_loss: 0.0294\n",
            "Epoch 429/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0292 - val_loss: 0.0292\n",
            "Epoch 430/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0286 - val_loss: 0.0288\n",
            "Epoch 431/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0292 - val_loss: 0.0292\n",
            "Epoch 432/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0283 - val_loss: 0.0292\n",
            "Epoch 433/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0293 - val_loss: 0.0296\n",
            "Epoch 434/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0292 - val_loss: 0.0293\n",
            "Epoch 435/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0292 - val_loss: 0.0295\n",
            "Epoch 436/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0287 - val_loss: 0.0289\n",
            "Epoch 437/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0284 - val_loss: 0.0295\n",
            "Epoch 438/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0287 - val_loss: 0.0290\n",
            "Epoch 439/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0283 - val_loss: 0.0288\n",
            "Epoch 440/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0286 - val_loss: 0.0288\n",
            "Epoch 441/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0290 - val_loss: 0.0292\n",
            "Epoch 442/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0279 - val_loss: 0.0291\n",
            "Epoch 443/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0288 - val_loss: 0.0294\n",
            "Epoch 444/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0284 - val_loss: 0.0287\n",
            "Epoch 445/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0283 - val_loss: 0.0301\n",
            "Epoch 446/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0288 - val_loss: 0.0321\n",
            "Epoch 447/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0283 - val_loss: 0.0287\n",
            "Epoch 448/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0289 - val_loss: 0.0324\n",
            "Epoch 449/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0289 - val_loss: 0.0303\n",
            "Epoch 450/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0284 - val_loss: 0.0289\n",
            "Epoch 451/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0280 - val_loss: 0.0287\n",
            "Epoch 452/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0283 - val_loss: 0.0287\n",
            "Epoch 453/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0293 - val_loss: 0.0295\n",
            "Epoch 454/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0284 - val_loss: 0.0299\n",
            "Epoch 455/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0281 - val_loss: 0.0316\n",
            "Epoch 456/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0287 - val_loss: 0.0286\n",
            "Epoch 457/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0281 - val_loss: 0.0286\n",
            "Epoch 458/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0292 - val_loss: 0.0289\n",
            "Epoch 459/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0285 - val_loss: 0.0290\n",
            "Epoch 460/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0279 - val_loss: 0.0287\n",
            "Epoch 461/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0281 - val_loss: 0.0287\n",
            "Epoch 462/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0291 - val_loss: 0.0290\n",
            "Epoch 463/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0283 - val_loss: 0.0289\n",
            "Epoch 464/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0281 - val_loss: 0.0292\n",
            "Epoch 465/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0283 - val_loss: 0.0294\n",
            "Epoch 466/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0281 - val_loss: 0.0362\n",
            "Epoch 467/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0286 - val_loss: 0.0292\n",
            "Epoch 468/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0288 - val_loss: 0.0293\n",
            "Epoch 469/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0281 - val_loss: 0.0293\n",
            "Epoch 470/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0286 - val_loss: 0.0290\n",
            "Epoch 471/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0280 - val_loss: 0.0285\n",
            "Epoch 472/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0286 - val_loss: 0.0285\n",
            "Epoch 473/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0282 - val_loss: 0.0286\n",
            "Epoch 474/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0281 - val_loss: 0.0289\n",
            "Epoch 475/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0285 - val_loss: 0.0285\n",
            "Epoch 476/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0284 - val_loss: 0.0286\n",
            "Epoch 477/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0280 - val_loss: 0.0284\n",
            "Epoch 478/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0282 - val_loss: 0.0300\n",
            "Epoch 479/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0277 - val_loss: 0.0284\n",
            "Epoch 480/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0281 - val_loss: 0.0286\n",
            "Epoch 481/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0286 - val_loss: 0.0285\n",
            "Epoch 482/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0281 - val_loss: 0.0283\n",
            "Epoch 483/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0280 - val_loss: 0.0284\n",
            "Epoch 484/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0280 - val_loss: 0.0295\n",
            "Epoch 485/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0284 - val_loss: 0.0293\n",
            "Epoch 486/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0282 - val_loss: 0.0294\n",
            "Epoch 487/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0278 - val_loss: 0.0283\n",
            "Epoch 488/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0283 - val_loss: 0.0298\n",
            "Epoch 489/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0280 - val_loss: 0.0285\n",
            "Epoch 490/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0286 - val_loss: 0.0283\n",
            "Epoch 491/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0286 - val_loss: 0.0282\n",
            "Epoch 492/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0280 - val_loss: 0.0291\n",
            "Epoch 493/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0277 - val_loss: 0.0285\n",
            "Epoch 494/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0279 - val_loss: 0.0282\n",
            "Epoch 495/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0278 - val_loss: 0.0287\n",
            "Epoch 496/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0282 - val_loss: 0.0342\n",
            "Epoch 497/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0280 - val_loss: 0.0284\n",
            "Epoch 498/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0274 - val_loss: 0.0282\n",
            "Epoch 499/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0294 - val_loss: 0.0285\n",
            "Epoch 500/500\n",
            "2896/2896 [==============================] - 5s 2ms/step - loss: 0.0278 - val_loss: 0.0284\n",
            "Calculating threshold\n",
            "Calculating MSE on optimization set...\n",
            "mean is 0.02843\n",
            "min is 0.00002\n",
            "max is 28.14064\n",
            "std is 0.12588\n",
            "Calculated threshold is 0.1543085172932212\n",
            "Calculating MSE on test set...\n",
            "4467 false positives on dataset without attacks with size 185311\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fjXXkm4_8_Zl",
        "outputId": "4dffa7b1-8398-4399-b7a1-04da0165abd3"
      },
      "source": [
        "!pip install lime"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting lime\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/86/91a13127d83d793ecb50eb75e716f76e6eda809b6803c5a4ff462339789e/lime-0.2.0.1.tar.gz (275kB)\n",
            "\r\u001b[K     |█▏                              | 10kB 15.5MB/s eta 0:00:01\r\u001b[K     |██▍                             | 20kB 20.9MB/s eta 0:00:01\r\u001b[K     |███▋                            | 30kB 10.7MB/s eta 0:00:01\r\u001b[K     |████▊                           | 40kB 9.1MB/s eta 0:00:01\r\u001b[K     |██████                          | 51kB 7.4MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 61kB 7.3MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 71kB 7.2MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 81kB 7.9MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 92kB 8.0MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 102kB 8.2MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 112kB 8.2MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 122kB 8.2MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 133kB 8.2MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 143kB 8.2MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 153kB 8.2MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 163kB 8.2MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 174kB 8.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 184kB 8.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 194kB 8.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 204kB 8.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 215kB 8.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 225kB 8.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 235kB 8.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 245kB 8.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 256kB 8.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 266kB 8.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 276kB 8.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from lime) (3.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from lime) (1.19.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from lime) (1.4.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from lime) (4.41.1)\n",
            "Requirement already satisfied: scikit-learn>=0.18 in /usr/local/lib/python3.7/dist-packages (from lime) (0.22.2.post1)\n",
            "Requirement already satisfied: scikit-image>=0.12 in /usr/local/lib/python3.7/dist-packages (from lime) (0.16.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->lime) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->lime) (1.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->lime) (2.8.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->lime) (0.10.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.18->lime) (1.0.1)\n",
            "Requirement already satisfied: pillow>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.12->lime) (7.1.2)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.12->lime) (2.4.1)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.12->lime) (2.5)\n",
            "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.12->lime) (1.1.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->lime) (1.15.0)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from networkx>=2.0->scikit-image>=0.12->lime) (4.4.2)\n",
            "Building wheels for collected packages: lime\n",
            "  Building wheel for lime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lime: filename=lime-0.2.0.1-cp37-none-any.whl size=283846 sha256=25f9096a6af3f51644688e85e86d8c4138ff92f701329c5bce95af98546d89dd\n",
            "  Stored in directory: /root/.cache/pip/wheels/4c/4f/a5/0bc765457bd41378bf3ce8d17d7495369d6e7ca3b712c60c89\n",
            "Successfully built lime\n",
            "Installing collected packages: lime\n",
            "Successfully installed lime-0.2.0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QzABYiAt90nU",
        "outputId": "7e7dd81f-26a2-422c-f029-6908466cefac"
      },
      "source": [
        "!pip install patool"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting patool\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/43/94/52243ddff508780dd2d8110964320ab4851134a55ab102285b46e740f76a/patool-1.12-py2.py3-none-any.whl (77kB)\n",
            "\r\u001b[K     |████▎                           | 10kB 15.6MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 20kB 22.1MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 30kB 11.5MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 40kB 9.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 51kB 7.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 61kB 7.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 71kB 7.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 81kB 4.8MB/s \n",
            "\u001b[?25hInstalling collected packages: patool\n",
            "Successfully installed patool-1.12\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "84FyUgLe99rH"
      },
      "source": [
        "from glob import iglob\n",
        "file_list = [f for f in iglob('/content/botnet-traffic-analysis/data/**/mirai_attacks.rar')]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jAspdOXK-6x0",
        "outputId": "61c5db5f-ead5-4b3d-f6a4-422db3e81fdc"
      },
      "source": [
        "for path in file_list:\n",
        "  new_dirname = path.split(\"/\")[-1].split(\".\")[0]\n",
        "  full_name = '/'.join(path.split(\"/\")[:-1]) + \"/\" + new_dirname\n",
        "  try:\n",
        "    os.mkdir(full_name)\n",
        "    patoolib.extract_archive(path, outdir=full_name)\n",
        "  except:\n",
        "    print(full_name, \"Has been created before already\")  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/botnet-traffic-analysis/data/Provision_PT_838_Security_Camera/mirai_attacks Has been created before already\n",
            "/content/botnet-traffic-analysis/data/Ecobee_Thermostat/mirai_attacks Has been created before already\n",
            "/content/botnet-traffic-analysis/data/SimpleHome_XCS7_1003_WHT_Security_Camera/mirai_attacks Has been created before already\n",
            "/content/botnet-traffic-analysis/data/SimpleHome_XCS7_1002_WHT_Security_Camera/mirai_attacks Has been created before already\n",
            "/content/botnet-traffic-analysis/data/Philips_B120N10_Baby_Monitor/mirai_attacks Has been created before already\n",
            "/content/botnet-traffic-analysis/data/Provision_PT_737E_Security_Camera/mirai_attacks Has been created before already\n",
            "/content/botnet-traffic-analysis/data/Danmini_Doorbell/mirai_attacks Has been created before already\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mVOU01kU_xNV",
        "outputId": "926ad227-45f6-4fd4-ad34-ea7151f695a4"
      },
      "source": [
        "!python3 /content/botnet-traffic-analysis/anomaly-detection/test.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-04-05 17:25:14.378952: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "tcmalloc: large alloc 3374931968 bytes == 0x55e409516000 @  0x7f32ec9511e7 0x7f32ea4d146e 0x7f32ea521c7b 0x7f32ea521d18 0x7f32ea5c9010 0x7f32ea5c973c 0x7f32ea5c985d 0x55e32815d2f8 0x7f32ea50eef7 0x55e32815afd7 0x55e32815ade0 0x55e3281ceac2 0x55e3281c9b0e 0x55e32815c77a 0x55e3281cb86a 0x55e3281c9e0d 0x55e32815c77a 0x55e3281cb86a 0x55e3281c9b0e 0x55e32815c77a 0x55e3281cb86a 0x55e32815c69a 0x55e3281cac9e 0x55e3281c9b0e 0x55e32815c77a 0x55e3281cb86a 0x55e32815c69a 0x55e3281caa45 0x55e32815c8f1 0x55e32809bf30 0x55e3281cc1e6\n",
            "tcmalloc: large alloc 3374931968 bytes == 0x55e4d27ac000 @  0x7f32ec9511e7 0x7f32ea4d146e 0x7f32ea521c7b 0x7f32ea521d97 0x7f32ea51b4a5 0x7f32ea5b8eab 0x55e32815b050 0x55e32824c99d 0x55e3281cefe9 0x55e3281c9b0e 0x55e32815c77a 0x55e3281cb86a 0x55e32815c69a 0x55e3281cac9e 0x55e3281c9b0e 0x55e32815c77a 0x55e3281cb86a 0x55e3281c9b0e 0x55e32815c77a 0x55e3281cee50 0x55e32815c69a 0x55e3281caa45 0x55e32815c8f1 0x55e32809bf30 0x55e3281cc1e6 0x55e3281c9b0e 0x55e3281c9813 0x55e328293592 0x55e32829390d 0x55e3282937b6 0x55e32826b103\n",
            "Testing\n",
            "Loading model\n",
            "2021-04-05 17:27:08.095274: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
            "2021-04-05 17:27:08.155192: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
            "2021-04-05 17:27:08.177812: E tensorflow/stream_executor/cuda/cuda_driver.cc:328] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "2021-04-05 17:27:08.181103: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (8201cda12089): /proc/driver/nvidia/version does not exist\n",
            "2021-04-05 17:27:08.187216: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX512F\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-04-05 17:27:08.187420: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
            "Calculated threshold is 0.19132518021021844\n",
            "2021-04-05 17:27:09.106436: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
            "2021-04-05 17:27:09.124292: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 1999995000 Hz\n",
            "Accuracy\n",
            "0.9922724500974038\n",
            "Recall\n",
            "0.9992876839475261\n",
            "Precision\n",
            "0.9854612023777214\n",
            "[[182579   2732]\n",
            " [   132 185179]]\n",
            "explaining with LIME\n",
            "Explaining for record nr 66894\n",
            "[('H_L0.01_weight > 100.15', 0.06472154686093477), ('MI_dir_L0.01_weight > 100.15', 0.05289335123835411), ('353.98 < MI_dir_L0.01_variance <= 1789.19', -0.039132370634611224), ('H_L0.01_mean <= 73.61', -0.03898842480871732), ('H_L0.1_mean <= 72.34', -0.034296526951970725), ('MI_dir_L0.1_mean <= 72.34', -0.03232756080495973), ('MI_dir_L1_mean <= 66.04', -0.02944781475429378), ('H_L1_mean <= 66.04', -0.028336766148308767), ('354.04 < H_L0.01_variance <= 1792.30', -0.027639161636779562), ('MI_dir_L0.01_mean <= 73.61', -0.027595394919761342)]\n",
            "Actual class\n",
            "357551    0\n",
            "Name: malicious, dtype: int64\n",
            "Explaining for record nr 239810\n",
            "[('MI_dir_L0.01_weight > 100.15', 0.06484185393175501), ('H_L0.01_weight > 100.15', 0.05950915415752457), ('MI_dir_L0.1_mean <= 72.34', -0.04729479494070045), ('H_L0.01_variance <= 354.04', -0.04590126155352203), ('H_L0.01_mean <= 73.61', -0.040564453680091504), ('66.04 < MI_dir_L1_mean <= 82.41', -0.03165866442548511), ('MI_dir_L0.01_mean <= 73.61', -0.03126092895622878), ('H_L0.1_mean <= 72.34', -0.028584246907383353), ('MI_dir_L0.01_variance <= 353.98', -0.025415958458395855), ('66.04 < H_L1_mean <= 82.41', -0.02208216890854124)]\n",
            "Actual class\n",
            "3100410    1\n",
            "Name: malicious, dtype: int64\n",
            "Explaining for record nr 299745\n",
            "[('H_L0.01_mean > 150.50', 0.08749929497640606), ('MI_dir_L0.01_mean > 150.50', 0.08528239583848914), ('MI_dir_L0.01_variance > 23398.77', 0.0802185998893115), ('H_L0.1_mean > 151.85', 0.07922204187407254), ('MI_dir_L0.1_mean > 151.85', 0.07257653448748645), ('H_L0.01_variance > 23398.77', 0.07241198564163004), ('MI_dir_L1_mean > 114.69', 0.06675402462738703), ('H_L1_mean > 114.69', 0.066236199887077), ('H_L0.01_weight > 100.15', 0.05781360115563891), ('MI_dir_L0.01_weight > 100.15', 0.05048571121682913)]\n",
            "Actual class\n",
            "633758    1\n",
            "Name: malicious, dtype: int64\n",
            "Explaining for record nr 196111\n",
            "[('H_L0.01_mean > 150.50', 0.07743874961554532), ('MI_dir_L0.01_mean > 150.50', 0.0771311597333191), ('MI_dir_L0.1_mean > 151.85', 0.074330186347008), ('H_L0.01_variance > 23398.77', 0.06951921229742834), ('MI_dir_L0.01_variance > 23398.77', 0.06943395419560426), ('H_L0.1_mean > 151.85', 0.06639818474454773), ('H_L1_mean > 114.69', 0.05504456398718462), ('H_L0.01_weight > 100.15', 0.0534323236296434), ('MI_dir_L1_mean > 114.69', 0.051390381055188984), ('MI_dir_L0.01_weight > 100.15', 0.04509948644444631)]\n",
            "Actual class\n",
            "3555324    1\n",
            "Name: malicious, dtype: int64\n",
            "Explaining for record nr 55596\n",
            "[('MI_dir_L0.01_mean > 150.50', 0.09561309262438736), ('H_L0.01_mean > 150.50', 0.09436583093685094), ('28.24 < MI_dir_L0.01_weight <= 38.13', -0.03980047133776872), ('1792.30 < H_L0.01_variance <= 23398.77', -0.038988059329835935), ('1789.19 < MI_dir_L0.01_variance <= 23398.77', -0.034369199500669224), ('86.58 < H_L0.1_mean <= 151.85', -0.03265479670208514), ('82.41 < MI_dir_L1_mean <= 114.69', -0.02644836830894368), ('82.41 < H_L1_mean <= 114.69', -0.026415020297932086), ('28.24 < H_L0.01_weight <= 38.13', -0.021503641714958027), ('86.58 < MI_dir_L0.1_mean <= 151.85', -0.01304975954836423)]\n",
            "Actual class\n",
            "215668    0\n",
            "Name: malicious, dtype: int64\n",
            "---------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rx6Xb3JJBECE",
        "outputId": "8b8c7518-fe5a-4f39-8449-b540a79a6111"
      },
      "source": [
        "os.chdir('/content')\n",
        "rootpath = '/content/botnet-traffic-analysis'\n",
        "\n",
        "!zip -r /content/botnet.zip /content/botnet-traffic-analysis"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  adding: content/botnet-traffic-analysis/ (stored 0%)\n",
            "  adding: content/botnet-traffic-analysis/jupyter/ (stored 0%)\n",
            "  adding: content/botnet-traffic-analysis/jupyter/anomaly_scores.csv (deflated 80%)\n",
            "  adding: content/botnet-traffic-analysis/jupyter/autoencoder_traffic.h5 (deflated 18%)\n",
            "  adding: content/botnet-traffic-analysis/jupyter/mirai_attack_type.ipynb (deflated 83%)\n",
            "  adding: content/botnet-traffic-analysis/jupyter/botnet_type2.ipynb (deflated 83%)\n",
            "  adding: content/botnet-traffic-analysis/jupyter/anomaly.ipynb (deflated 47%)\n",
            "  adding: content/botnet-traffic-analysis/jupyter/anomaly/ (stored 0%)\n",
            "  adding: content/botnet-traffic-analysis/jupyter/anomaly/events.out.tfevents.1546787810.Sergeis-MacBook-Pro.local (deflated 90%)\n",
            "  adding: content/botnet-traffic-analysis/jupyter/anomaly/events.out.tfevents.1546078960.Sergeis-MacBook-Pro.local (deflated 90%)\n",
            "  adding: content/botnet-traffic-analysis/jupyter/anomaly/anomaly9.h5 (deflated 87%)\n",
            "  adding: content/botnet-traffic-analysis/jupyter/anomaly/anomaly7.h5 (deflated 88%)\n",
            "  adding: content/botnet-traffic-analysis/jupyter/anomaly/events.out.tfevents.1546111234.Sergeis-MacBook-Pro.local (deflated 91%)\n",
            "  adding: content/botnet-traffic-analysis/jupyter/anomaly/anomaly14.h5 (deflated 81%)\n",
            "  adding: content/botnet-traffic-analysis/jupyter/anomaly/anomaly2.h5 (deflated 92%)\n",
            "  adding: content/botnet-traffic-analysis/jupyter/anomaly/anomaly1.h5 (deflated 93%)\n",
            "  adding: content/botnet-traffic-analysis/jupyter/anomaly/anomaly12.h5 (deflated 84%)\n",
            "  adding: content/botnet-traffic-analysis/jupyter/anomaly/events.out.tfevents.1546794128.Sergeis-MacBook-Pro.local (deflated 90%)\n",
            "  adding: content/botnet-traffic-analysis/jupyter/anomaly/anomaly6.h5 (deflated 90%)\n",
            "  adding: content/botnet-traffic-analysis/jupyter/anomaly/events.out.tfevents.1546789044.Sergeis-MacBook-Pro.local (deflated 90%)\n",
            "  adding: content/botnet-traffic-analysis/jupyter/anomaly/events.out.tfevents.1546787405.Sergeis-MacBook-Pro.local (deflated 90%)\n",
            "  adding: content/botnet-traffic-analysis/jupyter/anomaly/anomaly115.h5 (deflated 17%)\n",
            "  adding: content/botnet-traffic-analysis/jupyter/anomaly/anomaly100.h5 (deflated 20%)\n",
            "  adding: content/botnet-traffic-analysis/jupyter/anomaly/anomaly13.h5 (deflated 83%)\n",
            "  adding: content/botnet-traffic-analysis/jupyter/anomaly/events.out.tfevents.1546074992.Sergeis-MacBook-Pro.local (deflated 90%)\n",
            "  adding: content/botnet-traffic-analysis/jupyter/anomaly/events.out.tfevents.1546080371.Sergeis-MacBook-Pro.local (deflated 90%)\n",
            "  adding: content/botnet-traffic-analysis/jupyter/anomaly/anomaly5.h5 (deflated 90%)\n",
            "  adding: content/botnet-traffic-analysis/jupyter/anomaly/anomaly3.h5 (deflated 92%)\n",
            "  adding: content/botnet-traffic-analysis/jupyter/anomaly/anomaly10.h5 (deflated 85%)\n",
            "  adding: content/botnet-traffic-analysis/jupyter/anomaly/events.out.tfevents.1546889118.Sergeis-MacBook-Pro.local (deflated 90%)\n",
            "  adding: content/botnet-traffic-analysis/jupyter/anomaly/anomaly11.h5 (deflated 85%)\n",
            "  adding: content/botnet-traffic-analysis/jupyter/anomaly/anomaly16.h5 (deflated 79%)\n",
            "  adding: content/botnet-traffic-analysis/jupyter/anomaly/anomaly15.h5 (deflated 80%)\n",
            "  adding: content/botnet-traffic-analysis/jupyter/anomaly/events.out.tfevents.1546075370.Sergeis-MacBook-Pro.local (deflated 90%)\n",
            "  adding: content/botnet-traffic-analysis/jupyter/anomaly/events.out.tfevents.1546793504.Sergeis-MacBook-Pro.local (deflated 90%)\n",
            "  adding: content/botnet-traffic-analysis/jupyter/anomaly/events.out.tfevents.1546110982.Sergeis-MacBook-Pro.local (deflated 91%)\n",
            "  adding: content/botnet-traffic-analysis/jupyter/anomaly/anomaly8.h5 (deflated 87%)\n",
            "  adding: content/botnet-traffic-analysis/jupyter/anomaly/anomaly4.h5 (deflated 91%)\n",
            "  adding: content/botnet-traffic-analysis/jupyter/anomaly/events.out.tfevents.1546885519.Sergeis-MacBook-Pro.local (deflated 90%)\n",
            "  adding: content/botnet-traffic-analysis/jupyter/anomaly/events.out.tfevents.1546785701.Sergeis-MacBook-Pro.local (deflated 90%)\n",
            "  adding: content/botnet-traffic-analysis/jupyter/anomaly/anomaly17.h5 (deflated 78%)\n",
            "  adding: content/botnet-traffic-analysis/jupyter/anomaly/events.out.tfevents.1546787022.Sergeis-MacBook-Pro.local (deflated 90%)\n",
            "  adding: content/botnet-traffic-analysis/jupyter/anomaly/events.out.tfevents.1546110855.Sergeis-MacBook-Pro.local (deflated 91%)\n",
            "  adding: content/botnet-traffic-analysis/jupyter/anomaly/events.out.tfevents.1546111765.Sergeis-MacBook-Pro.local (deflated 91%)\n",
            "  adding: content/botnet-traffic-analysis/jupyter/atk_classification_scores.csv (deflated 79%)\n",
            "  adding: content/botnet-traffic-analysis/jupyter/classification_scores.csv (deflated 79%)\n",
            "  adding: content/botnet-traffic-analysis/jupyter/Anomaly_detection_stand_norm.ipynb (deflated 78%)\n",
            "  adding: content/botnet-traffic-analysis/jupyter/Anomaly_detection_standard_norm.ipynb (deflated 76%)\n",
            "  adding: content/botnet-traffic-analysis/jupyter/models/ (stored 0%)\n",
            "  adding: content/botnet-traffic-analysis/jupyter/models/model_atk_types3.h5 (deflated 86%)\n",
            "  adding: content/botnet-traffic-analysis/jupyter/models/model_70.h5 (deflated 73%)\n",
            "  adding: content/botnet-traffic-analysis/jupyter/models/model_9.h5 (deflated 86%)\n",
            "  adding: content/botnet-traffic-analysis/jupyter/models/model_27.h5 (deflated 31%)\n",
            "  adding: content/botnet-traffic-analysis/jupyter/models/model_80.h5 (deflated 72%)\n",
            "  adding: content/botnet-traffic-analysis/jupyter/models/model_100.h5 (deflated 68%)\n",
            "  adding: content/botnet-traffic-analysis/jupyter/models/model_2.h5 (deflated 87%)\n",
            "  adding: content/botnet-traffic-analysis/jupyter/models/model_atk_types10.h5 (deflated 85%)\n",
            "  adding: content/botnet-traffic-analysis/jupyter/models/model_60.h5 (deflated 76%)\n",
            "  adding: content/botnet-traffic-analysis/jupyter/models/model_atk_types_diff.h5 (deflated 71%)\n",
            "  adding: content/botnet-traffic-analysis/jupyter/models/model_4.h5 (deflated 86%)\n",
            "  adding: content/botnet-traffic-analysis/jupyter/models/model_atk_types6.h5 (deflated 86%)\n",
            "  adding: content/botnet-traffic-analysis/jupyter/models/model_115.h5 (deflated 66%)\n",
            "  adding: content/botnet-traffic-analysis/jupyter/models/model_atk_types7.h5 (deflated 86%)\n",
            "  adding: content/botnet-traffic-analysis/jupyter/models/model_40.h5 (deflated 79%)\n",
            "  adding: content/botnet-traffic-analysis/jupyter/models/model_22.h5 (deflated 32%)\n",
            "  adding: content/botnet-traffic-analysis/jupyter/models/model_atk_types5.h5 (deflated 85%)\n",
            "  adding: content/botnet-traffic-analysis/jupyter/models/model_7.h5 (deflated 85%)\n",
            "  adding: content/botnet-traffic-analysis/jupyter/models/model_30.h5 (deflated 82%)\n",
            "  adding: content/botnet-traffic-analysis/jupyter/models/model_atk_types2.h5 (deflated 86%)\n",
            "  adding: content/botnet-traffic-analysis/jupyter/models/model_20.h5 (deflated 83%)\n",
            "  adding: content/botnet-traffic-analysis/jupyter/models/model_90.h5 (deflated 70%)\n",
            "  adding: content/botnet-traffic-analysis/jupyter/models/model_atk_types4.h5 (deflated 86%)\n",
            "  adding: content/botnet-traffic-analysis/jupyter/models/model_5.h5 (deflated 86%)\n",
            "  adding: content/botnet-traffic-analysis/jupyter/models/model_12.h5 (deflated 33%)\n",
            "  adding: content/botnet-traffic-analysis/jupyter/models/model_17.h5 (deflated 33%)\n",
            "  adding: content/botnet-traffic-analysis/jupyter/models/model_atk_types.h5 (deflated 71%)\n",
            "  adding: content/botnet-traffic-analysis/jupyter/models/model_10.h5 (deflated 85%)\n",
            "  adding: content/botnet-traffic-analysis/jupyter/models/model_atk_types8.h5 (deflated 86%)\n",
            "  adding: content/botnet-traffic-analysis/jupyter/models/model_50.h5 (deflated 77%)\n",
            "  adding: content/botnet-traffic-analysis/jupyter/models/model_8.h5 (deflated 86%)\n",
            "  adding: content/botnet-traffic-analysis/jupyter/models/model_atk_types115.h5 (deflated 71%)\n",
            "  adding: content/botnet-traffic-analysis/jupyter/models/model_3.h5 (deflated 86%)\n",
            "  adding: content/botnet-traffic-analysis/jupyter/models/model_1.h5 (deflated 87%)\n",
            "  adding: content/botnet-traffic-analysis/jupyter/models/model_6.h5 (deflated 85%)\n",
            "  adding: content/botnet-traffic-analysis/jupyter/mirai_attack_type2.ipynb (deflated 83%)\n",
            "  adding: content/botnet-traffic-analysis/jupyter/Anomaly_detection_minmax_scaling2.ipynb (deflated 82%)\n",
            "  adding: content/botnet-traffic-analysis/jupyter/Anomaly_detection_minmax_scaling.ipynb (deflated 80%)\n",
            "  adding: content/botnet-traffic-analysis/jupyter/classification_model.png (deflated 7%)\n",
            "  adding: content/botnet-traffic-analysis/jupyter/fisher.ipynb (deflated 74%)\n",
            "  adding: content/botnet-traffic-analysis/jupyter/utils.py (deflated 54%)\n",
            "  adding: content/botnet-traffic-analysis/jupyter/model.png (deflated 8%)\n",
            "  adding: content/botnet-traffic-analysis/jupyter/botnet_type.ipynb (deflated 83%)\n",
            "  adding: content/botnet-traffic-analysis/classification/ (stored 0%)\n",
            "  adding: content/botnet-traffic-analysis/classification/exp115.csv (deflated 83%)\n",
            "  adding: content/botnet-traffic-analysis/classification/lime/ (stored 0%)\n",
            "  adding: content/botnet-traffic-analysis/classification/lime/explanation8.html (deflated 80%)\n",
            "  adding: content/botnet-traffic-analysis/classification/lime/explanation9.html (deflated 80%)\n",
            "  adding: content/botnet-traffic-analysis/classification/lime/explanation3.html (deflated 80%)\n",
            "  adding: content/botnet-traffic-analysis/classification/lime/explanation5.html (deflated 80%)\n",
            "  adding: content/botnet-traffic-analysis/classification/lime/explanation4.html (deflated 80%)\n",
            "  adding: content/botnet-traffic-analysis/classification/lime/explanation2.html (deflated 80%)\n",
            "  adding: content/botnet-traffic-analysis/classification/lime/explanation6.html (deflated 80%)\n",
            "  adding: content/botnet-traffic-analysis/classification/lime/explanation1.html (deflated 80%)\n",
            "  adding: content/botnet-traffic-analysis/classification/lime/explanation0.html (deflated 80%)\n",
            "  adding: content/botnet-traffic-analysis/classification/lime/explanation7.html (deflated 80%)\n",
            "  adding: content/botnet-traffic-analysis/classification/test.py (deflated 59%)\n",
            "  adding: content/botnet-traffic-analysis/classification/experiment.py (deflated 56%)\n",
            "  adding: content/botnet-traffic-analysis/classification/exp9.csv (deflated 85%)\n",
            "  adding: content/botnet-traffic-analysis/classification/README.md (deflated 54%)\n",
            "  adding: content/botnet-traffic-analysis/classification/train.py (deflated 66%)\n",
            "  adding: content/botnet-traffic-analysis/classification/models/ (stored 0%)\n",
            "  adding: content/botnet-traffic-analysis/classification/models/scaler_4.sav (deflated 21%)\n",
            "  adding: content/botnet-traffic-analysis/classification/models/model_9.h5 (deflated 16%)\n",
            "  adding: content/botnet-traffic-analysis/classification/models/scaler_115.sav (deflated 10%)\n",
            "  adding: content/botnet-traffic-analysis/classification/models/scaler_3.sav (deflated 22%)\n",
            "  adding: content/botnet-traffic-analysis/classification/models/model_2.h5 (deflated 15%)\n",
            "  adding: content/botnet-traffic-analysis/classification/models/scaler_9.sav (deflated 19%)\n",
            "  adding: content/botnet-traffic-analysis/classification/models/model_4.h5 (deflated 16%)\n",
            "  adding: content/botnet-traffic-analysis/classification/models/model.h5 (deflated 23%)\n",
            "  adding: content/botnet-traffic-analysis/classification/models/model_115.h5 (deflated 14%)\n",
            "  adding: content/botnet-traffic-analysis/classification/models/model_5.h5 (deflated 16%)\n",
            "  adding: content/botnet-traffic-analysis/classification/models/model_3.h5 (deflated 16%)\n",
            "  adding: content/botnet-traffic-analysis/classification/logs/ (stored 0%)\n",
            "  adding: content/botnet-traffic-analysis/classification/logs/events.out.tfevents.1544638353.Sergeis-MacBook-Pro.local (deflated 89%)\n",
            "  adding: content/botnet-traffic-analysis/classification/logs/events.out.tfevents.1544630562.Sergeis-MBP (deflated 89%)\n",
            "  adding: content/botnet-traffic-analysis/classification/logs/events.out.tfevents.1544630067.Sergeis-MBP (deflated 89%)\n",
            "  adding: content/botnet-traffic-analysis/classification/logs/events.out.tfevents.1544627919.Sergeis-MBP (deflated 89%)\n",
            "  adding: content/botnet-traffic-analysis/classification/logs/events.out.tfevents.1544629623.Sergeis-MBP (deflated 89%)\n",
            "  adding: content/botnet-traffic-analysis/classification/logs/events.out.tfevents.1544634467.Sergeis-MacBook-Pro.local (deflated 89%)\n",
            "  adding: content/botnet-traffic-analysis/classification/logs/events.out.tfevents.1542227635.ATHENA (deflated 89%)\n",
            "  adding: content/botnet-traffic-analysis/classification/logs/events.out.tfevents.1544637570.Sergeis-MacBook-Pro.local (deflated 89%)\n",
            "  adding: content/botnet-traffic-analysis/classification/logs/events.out.tfevents.1544636700.Sergeis-MacBook-Pro.local (deflated 89%)\n",
            "  adding: content/botnet-traffic-analysis/classification/logs/events.out.tfevents.1544628893.Sergeis-MBP (deflated 89%)\n",
            "  adding: content/botnet-traffic-analysis/classification/exp4.csv (deflated 85%)\n",
            "  adding: content/botnet-traffic-analysis/classification/exp3.csv (deflated 83%)\n",
            "  adding: content/botnet-traffic-analysis/data/ (stored 0%)\n",
            "  adding: content/botnet-traffic-analysis/data/Provision_PT_838_Security_Camera/ (stored 0%)\n",
            "  adding: content/botnet-traffic-analysis/data/Provision_PT_838_Security_Camera/gafgyt_attacks.rar (deflated 1%)\n",
            "  adding: content/botnet-traffic-analysis/data/Provision_PT_838_Security_Camera/mirai_attacks.rar (deflated 0%)\n",
            "  adding: content/botnet-traffic-analysis/data/Provision_PT_838_Security_Camera/benign_traffic.csv (deflated 67%)\n",
            "  adding: content/botnet-traffic-analysis/data/Provision_PT_838_Security_Camera/mirai_attacks/ (stored 0%)\n",
            "  adding: content/botnet-traffic-analysis/data/Provision_PT_838_Security_Camera/mirai_attacks/syn.csv (deflated 76%)\n",
            "  adding: content/botnet-traffic-analysis/data/Provision_PT_838_Security_Camera/mirai_attacks/udp.csv (deflated 75%)\n",
            "  adding: content/botnet-traffic-analysis/data/Provision_PT_838_Security_Camera/mirai_attacks/scan.csv (deflated 84%)\n",
            "  adding: content/botnet-traffic-analysis/data/Provision_PT_838_Security_Camera/mirai_attacks/udpplain.csv (deflated 77%)\n",
            "  adding: content/botnet-traffic-analysis/data/Provision_PT_838_Security_Camera/mirai_attacks/ack.csv (deflated 75%)\n",
            "  adding: content/botnet-traffic-analysis/data/Ennio_Doorbell/ (stored 0%)\n",
            "  adding: content/botnet-traffic-analysis/data/Ennio_Doorbell/gafgyt_attacks.rar (deflated 1%)\n",
            "  adding: content/botnet-traffic-analysis/data/Ennio_Doorbell/benign_traffic.csv (deflated 71%)\n",
            "  adding: content/botnet-traffic-analysis/data/Ecobee_Thermostat/ (stored 0%)\n",
            "  adding: content/botnet-traffic-analysis/data/Ecobee_Thermostat/gafgyt_attacks.rar (deflated 1%)\n",
            "  adding: content/botnet-traffic-analysis/data/Ecobee_Thermostat/mirai_attacks.rar (deflated 0%)\n",
            "  adding: content/botnet-traffic-analysis/data/Ecobee_Thermostat/benign_traffic.csv (deflated 75%)\n",
            "  adding: content/botnet-traffic-analysis/data/Ecobee_Thermostat/mirai_attacks/ (stored 0%)\n",
            "  adding: content/botnet-traffic-analysis/data/Ecobee_Thermostat/mirai_attacks/syn.csv (deflated 74%)\n",
            "  adding: content/botnet-traffic-analysis/data/Ecobee_Thermostat/mirai_attacks/udp.csv (deflated 74%)\n",
            "  adding: content/botnet-traffic-analysis/data/Ecobee_Thermostat/mirai_attacks/scan.csv (deflated 85%)\n",
            "  adding: content/botnet-traffic-analysis/data/Ecobee_Thermostat/mirai_attacks/udpplain.csv (deflated 75%)\n",
            "  adding: content/botnet-traffic-analysis/data/Ecobee_Thermostat/mirai_attacks/ack.csv (deflated 73%)\n",
            "  adding: content/botnet-traffic-analysis/data/SimpleHome_XCS7_1003_WHT_Security_Camera/ (stored 0%)\n",
            "  adding: content/botnet-traffic-analysis/data/SimpleHome_XCS7_1003_WHT_Security_Camera/gafgyt_attacks.rar (deflated 1%)\n",
            "  adding: content/botnet-traffic-analysis/data/SimpleHome_XCS7_1003_WHT_Security_Camera/mirai_attacks.rar (deflated 0%)\n",
            "  adding: content/botnet-traffic-analysis/data/SimpleHome_XCS7_1003_WHT_Security_Camera/benign_traffic.csv (deflated 79%)\n",
            "  adding: content/botnet-traffic-analysis/data/SimpleHome_XCS7_1003_WHT_Security_Camera/mirai_attacks/ (stored 0%)\n",
            "  adding: content/botnet-traffic-analysis/data/SimpleHome_XCS7_1003_WHT_Security_Camera/mirai_attacks/syn.csv (deflated 74%)\n",
            "  adding: content/botnet-traffic-analysis/data/SimpleHome_XCS7_1003_WHT_Security_Camera/mirai_attacks/udp.csv (deflated 74%)\n",
            "  adding: content/botnet-traffic-analysis/data/SimpleHome_XCS7_1003_WHT_Security_Camera/mirai_attacks/scan.csv (deflated 86%)\n",
            "  adding: content/botnet-traffic-analysis/data/SimpleHome_XCS7_1003_WHT_Security_Camera/mirai_attacks/udpplain.csv (deflated 75%)\n",
            "  adding: content/botnet-traffic-analysis/data/SimpleHome_XCS7_1003_WHT_Security_Camera/mirai_attacks/ack.csv (deflated 73%)\n",
            "  adding: content/botnet-traffic-analysis/data/SimpleHome_XCS7_1002_WHT_Security_Camera/ (stored 0%)\n",
            "  adding: content/botnet-traffic-analysis/data/SimpleHome_XCS7_1002_WHT_Security_Camera/gafgyt_attacks.rar (deflated 1%)\n",
            "  adding: content/botnet-traffic-analysis/data/SimpleHome_XCS7_1002_WHT_Security_Camera/mirai_attacks.rar (deflated 0%)\n",
            "  adding: content/botnet-traffic-analysis/data/SimpleHome_XCS7_1002_WHT_Security_Camera/benign_traffic.csv (deflated 72%)\n",
            "  adding: content/botnet-traffic-analysis/data/SimpleHome_XCS7_1002_WHT_Security_Camera/mirai_attacks/ (stored 0%)\n",
            "  adding: content/botnet-traffic-analysis/data/SimpleHome_XCS7_1002_WHT_Security_Camera/mirai_attacks/syn.csv (deflated 74%)\n",
            "  adding: content/botnet-traffic-analysis/data/SimpleHome_XCS7_1002_WHT_Security_Camera/mirai_attacks/udp.csv (deflated 74%)\n",
            "  adding: content/botnet-traffic-analysis/data/SimpleHome_XCS7_1002_WHT_Security_Camera/mirai_attacks/scan.csv (deflated 86%)\n",
            "  adding: content/botnet-traffic-analysis/data/SimpleHome_XCS7_1002_WHT_Security_Camera/mirai_attacks/udpplain.csv (deflated 75%)\n",
            "  adding: content/botnet-traffic-analysis/data/SimpleHome_XCS7_1002_WHT_Security_Camera/mirai_attacks/ack.csv (deflated 73%)\n",
            "  adding: content/botnet-traffic-analysis/data/Philips_B120N10_Baby_Monitor/ (stored 0%)\n",
            "  adding: content/botnet-traffic-analysis/data/Philips_B120N10_Baby_Monitor/gafgyt_attacks.rar (deflated 1%)\n",
            "  adding: content/botnet-traffic-analysis/data/Philips_B120N10_Baby_Monitor/mirai_attacks.rar (deflated 0%)\n",
            "  adding: content/botnet-traffic-analysis/data/Philips_B120N10_Baby_Monitor/benign_traffic.csv (deflated 65%)\n",
            "  adding: content/botnet-traffic-analysis/data/Philips_B120N10_Baby_Monitor/mirai_attacks/ (stored 0%)\n",
            "  adding: content/botnet-traffic-analysis/data/Philips_B120N10_Baby_Monitor/mirai_attacks/syn.csv (deflated 75%)\n",
            "  adding: content/botnet-traffic-analysis/data/Philips_B120N10_Baby_Monitor/mirai_attacks/udp.csv (deflated 75%)\n",
            "  adding: content/botnet-traffic-analysis/data/Philips_B120N10_Baby_Monitor/mirai_attacks/scan.csv (deflated 85%)\n",
            "  adding: content/botnet-traffic-analysis/data/Philips_B120N10_Baby_Monitor/mirai_attacks/udpplain.csv (deflated 77%)\n",
            "  adding: content/botnet-traffic-analysis/data/Philips_B120N10_Baby_Monitor/mirai_attacks/ack.csv (deflated 75%)\n",
            "  adding: content/botnet-traffic-analysis/data/Samsung_SNH_1011_N_Webcam/ (stored 0%)\n",
            "  adding: content/botnet-traffic-analysis/data/Samsung_SNH_1011_N_Webcam/gafgyt_attacks.rar (deflated 1%)\n",
            "  adding: content/botnet-traffic-analysis/data/Samsung_SNH_1011_N_Webcam/benign_traffic.csv (deflated 76%)\n",
            "  adding: content/botnet-traffic-analysis/data/Provision_PT_737E_Security_Camera/ (stored 0%)\n",
            "  adding: content/botnet-traffic-analysis/data/Provision_PT_737E_Security_Camera/gafgyt_attacks.rar (deflated 1%)\n",
            "  adding: content/botnet-traffic-analysis/data/Provision_PT_737E_Security_Camera/mirai_attacks.rar (deflated 0%)\n",
            "  adding: content/botnet-traffic-analysis/data/Provision_PT_737E_Security_Camera/benign_traffic.csv (deflated 70%)\n",
            "  adding: content/botnet-traffic-analysis/data/Provision_PT_737E_Security_Camera/mirai_attacks/ (stored 0%)\n",
            "  adding: content/botnet-traffic-analysis/data/Provision_PT_737E_Security_Camera/mirai_attacks/syn.csv (deflated 76%)\n",
            "  adding: content/botnet-traffic-analysis/data/Provision_PT_737E_Security_Camera/mirai_attacks/udp.csv (deflated 75%)\n",
            "  adding: content/botnet-traffic-analysis/data/Provision_PT_737E_Security_Camera/mirai_attacks/scan.csv (deflated 84%)\n",
            "  adding: content/botnet-traffic-analysis/data/Provision_PT_737E_Security_Camera/mirai_attacks/udpplain.csv (deflated 77%)\n",
            "  adding: content/botnet-traffic-analysis/data/Provision_PT_737E_Security_Camera/mirai_attacks/ack.csv (deflated 75%)\n",
            "  adding: content/botnet-traffic-analysis/data/Danmini_Doorbell/ (stored 0%)\n",
            "  adding: content/botnet-traffic-analysis/data/Danmini_Doorbell/gafgyt_attacks.rar (deflated 1%)\n",
            "  adding: content/botnet-traffic-analysis/data/Danmini_Doorbell/mirai_attacks.rar (deflated 0%)\n",
            "  adding: content/botnet-traffic-analysis/data/Danmini_Doorbell/benign_traffic.csv (deflated 75%)\n",
            "  adding: content/botnet-traffic-analysis/data/Danmini_Doorbell/mirai_attacks/ (stored 0%)\n",
            "  adding: content/botnet-traffic-analysis/data/Danmini_Doorbell/mirai_attacks/syn.csv (deflated 75%)\n",
            "  adding: content/botnet-traffic-analysis/data/Danmini_Doorbell/mirai_attacks/udp.csv (deflated 75%)\n",
            "  adding: content/botnet-traffic-analysis/data/Danmini_Doorbell/mirai_attacks/scan.csv (deflated 85%)\n",
            "  adding: content/botnet-traffic-analysis/data/Danmini_Doorbell/mirai_attacks/udpplain.csv (deflated 77%)\n",
            "  adding: content/botnet-traffic-analysis/data/Danmini_Doorbell/mirai_attacks/ack.csv (deflated 75%)\n",
            "  adding: content/botnet-traffic-analysis/fisher2.csv (deflated 70%)\n",
            "  adding: content/botnet-traffic-analysis/.gitignore (deflated 6%)\n",
            "  adding: content/botnet-traffic-analysis/README.md (deflated 43%)\n",
            "  adding: content/botnet-traffic-analysis/threshold_10 (stored 0%)\n",
            "  adding: content/botnet-traffic-analysis/fisher.csv (deflated 74%)\n",
            "  adding: content/botnet-traffic-analysis/logs/ (stored 0%)\n",
            "  adding: content/botnet-traffic-analysis/logs/validation/ (stored 0%)\n",
            "  adding: content/botnet-traffic-analysis/logs/validation/events.out.tfevents.1617637569.8201cda12089.234.3696.v2 (deflated 64%)\n",
            "  adding: content/botnet-traffic-analysis/logs/train/ (stored 0%)\n",
            "  adding: content/botnet-traffic-analysis/logs/train/plugins/ (stored 0%)\n",
            "  adding: content/botnet-traffic-analysis/logs/train/plugins/profile/ (stored 0%)\n",
            "  adding: content/botnet-traffic-analysis/logs/train/plugins/profile/2021_04_05_15_46_06/ (stored 0%)\n",
            "  adding: content/botnet-traffic-analysis/logs/train/plugins/profile/2021_04_05_15_46_06/8201cda12089.input_pipeline.pb (deflated 57%)\n",
            "  adding: content/botnet-traffic-analysis/logs/train/plugins/profile/2021_04_05_15_46_06/8201cda12089.kernel_stats.pb (stored 0%)\n",
            "  adding: content/botnet-traffic-analysis/logs/train/plugins/profile/2021_04_05_15_46_06/8201cda12089.xplane.pb (deflated 67%)\n",
            "  adding: content/botnet-traffic-analysis/logs/train/plugins/profile/2021_04_05_15_46_06/8201cda12089.memory_profile.json.gz (stored 0%)\n",
            "  adding: content/botnet-traffic-analysis/logs/train/plugins/profile/2021_04_05_15_46_06/8201cda12089.trace.json.gz (stored 0%)\n",
            "  adding: content/botnet-traffic-analysis/logs/train/plugins/profile/2021_04_05_15_46_06/8201cda12089.tensorflow_stats.pb (deflated 73%)\n",
            "  adding: content/botnet-traffic-analysis/logs/train/plugins/profile/2021_04_05_15_46_06/8201cda12089.overview_page.pb (deflated 61%)\n",
            "  adding: content/botnet-traffic-analysis/logs/train/events.out.tfevents.1617637566.8201cda12089.profile-empty (deflated 5%)\n",
            "  adding: content/botnet-traffic-analysis/logs/train/events.out.tfevents.1617637565.8201cda12089.234.266.v2 (deflated 79%)\n",
            "  adding: content/botnet-traffic-analysis/requirements.txt (deflated 13%)\n",
            "  adding: content/botnet-traffic-analysis/thesis.pdf (deflated 9%)\n",
            "  adding: content/botnet-traffic-analysis/.git/ (stored 0%)\n",
            "  adding: content/botnet-traffic-analysis/.git/hooks/ (stored 0%)\n",
            "  adding: content/botnet-traffic-analysis/.git/hooks/post-update.sample (deflated 27%)\n",
            "  adding: content/botnet-traffic-analysis/.git/hooks/pre-push.sample (deflated 50%)\n",
            "  adding: content/botnet-traffic-analysis/.git/hooks/fsmonitor-watchman.sample (deflated 53%)\n",
            "  adding: content/botnet-traffic-analysis/.git/hooks/update.sample (deflated 68%)\n",
            "  adding: content/botnet-traffic-analysis/.git/hooks/applypatch-msg.sample (deflated 42%)\n",
            "  adding: content/botnet-traffic-analysis/.git/hooks/pre-commit.sample (deflated 43%)\n",
            "  adding: content/botnet-traffic-analysis/.git/hooks/pre-receive.sample (deflated 40%)\n",
            "  adding: content/botnet-traffic-analysis/.git/hooks/pre-applypatch.sample (deflated 38%)\n",
            "  adding: content/botnet-traffic-analysis/.git/hooks/prepare-commit-msg.sample (deflated 50%)\n",
            "  adding: content/botnet-traffic-analysis/.git/hooks/pre-rebase.sample (deflated 59%)\n",
            "  adding: content/botnet-traffic-analysis/.git/hooks/commit-msg.sample (deflated 44%)\n",
            "  adding: content/botnet-traffic-analysis/.git/config (deflated 34%)\n",
            "  adding: content/botnet-traffic-analysis/.git/index (deflated 62%)\n",
            "  adding: content/botnet-traffic-analysis/.git/HEAD (stored 0%)\n",
            "  adding: content/botnet-traffic-analysis/.git/packed-refs (deflated 10%)\n",
            "  adding: content/botnet-traffic-analysis/.git/objects/ (stored 0%)\n",
            "  adding: content/botnet-traffic-analysis/.git/objects/pack/ (stored 0%)\n",
            "  adding: content/botnet-traffic-analysis/.git/objects/pack/pack-09024d746ee40ce9aef7d0a762f06fa61f385a50.pack (deflated 0%)\n",
            "  adding: content/botnet-traffic-analysis/.git/objects/pack/pack-09024d746ee40ce9aef7d0a762f06fa61f385a50.idx (deflated 9%)\n",
            "  adding: content/botnet-traffic-analysis/.git/objects/info/ (stored 0%)\n",
            "  adding: content/botnet-traffic-analysis/.git/branches/ (stored 0%)\n",
            "  adding: content/botnet-traffic-analysis/.git/refs/ (stored 0%)\n",
            "  adding: content/botnet-traffic-analysis/.git/refs/tags/ (stored 0%)\n",
            "  adding: content/botnet-traffic-analysis/.git/refs/heads/ (stored 0%)\n",
            "  adding: content/botnet-traffic-analysis/.git/refs/heads/master (stored 0%)\n",
            "  adding: content/botnet-traffic-analysis/.git/refs/remotes/ (stored 0%)\n",
            "  adding: content/botnet-traffic-analysis/.git/refs/remotes/origin/ (stored 0%)\n",
            "  adding: content/botnet-traffic-analysis/.git/refs/remotes/origin/HEAD (stored 0%)\n",
            "  adding: content/botnet-traffic-analysis/.git/logs/ (stored 0%)\n",
            "  adding: content/botnet-traffic-analysis/.git/logs/HEAD (deflated 28%)\n",
            "  adding: content/botnet-traffic-analysis/.git/logs/refs/ (stored 0%)\n",
            "  adding: content/botnet-traffic-analysis/.git/logs/refs/heads/ (stored 0%)\n",
            "  adding: content/botnet-traffic-analysis/.git/logs/refs/heads/master (deflated 28%)\n",
            "  adding: content/botnet-traffic-analysis/.git/logs/refs/remotes/ (stored 0%)\n",
            "  adding: content/botnet-traffic-analysis/.git/logs/refs/remotes/origin/ (stored 0%)\n",
            "  adding: content/botnet-traffic-analysis/.git/logs/refs/remotes/origin/HEAD (deflated 28%)\n",
            "  adding: content/botnet-traffic-analysis/.git/description (deflated 14%)\n",
            "  adding: content/botnet-traffic-analysis/.git/info/ (stored 0%)\n",
            "  adding: content/botnet-traffic-analysis/.git/info/exclude (deflated 28%)\n",
            "  adding: content/botnet-traffic-analysis/config.json (deflated 73%)\n",
            "  adding: content/botnet-traffic-analysis/.vscode/ (stored 0%)\n",
            "  adding: content/botnet-traffic-analysis/.vscode/settings.json (deflated 13%)\n",
            "  adding: content/botnet-traffic-analysis/download_data.py (deflated 71%)\n",
            "  adding: content/botnet-traffic-analysis/anomaly-detection/ (stored 0%)\n",
            "  adding: content/botnet-traffic-analysis/anomaly-detection/lime/ (stored 0%)\n",
            "  adding: content/botnet-traffic-analysis/anomaly-detection/lime/explanation3.html (deflated 80%)\n",
            "  adding: content/botnet-traffic-analysis/anomaly-detection/lime/explanation.html (deflated 81%)\n",
            "  adding: content/botnet-traffic-analysis/anomaly-detection/lime/explanation4.html (deflated 80%)\n",
            "  adding: content/botnet-traffic-analysis/anomaly-detection/lime/explanation2.html (deflated 80%)\n",
            "  adding: content/botnet-traffic-analysis/anomaly-detection/lime/explanation1.html (deflated 80%)\n",
            "  adding: content/botnet-traffic-analysis/anomaly-detection/lime/explanation0.html (deflated 80%)\n",
            "  adding: content/botnet-traffic-analysis/anomaly-detection/test.py (deflated 65%)\n",
            "  adding: content/botnet-traffic-analysis/anomaly-detection/README.md (deflated 54%)\n",
            "  adding: content/botnet-traffic-analysis/anomaly-detection/train.py (deflated 64%)\n",
            "  adding: content/botnet-traffic-analysis/anomaly-detection/threshold_10 (stored 0%)\n",
            "  adding: content/botnet-traffic-analysis/anomaly-detection/models/ (stored 0%)\n",
            "  adding: content/botnet-traffic-analysis/anomaly-detection/models/model.h5 (deflated 19%)\n",
            "  adding: content/botnet-traffic-analysis/anomaly-detection/models/model_10.h5 (deflated 89%)\n",
            "  adding: content/botnet-traffic-analysis/anomaly-detection/logs/ (stored 0%)\n",
            "  adding: content/botnet-traffic-analysis/anomaly-detection/logs/events.out.tfevents.1542219705.ATHENA (deflated 87%)\n",
            "  adding: content/botnet-traffic-analysis/anomaly-detection/logs/events.out.tfevents.1542225012.ATHENA (deflated 87%)\n",
            "  adding: content/botnet-traffic-analysis/anomaly-detection/logs/events.out.tfevents.1542221940.ATHENA (deflated 88%)\n",
            "  adding: content/botnet-traffic-analysis/anomaly-detection/logs/events.out.tfevents.1542220969.ATHENA (deflated 88%)\n",
            "  adding: content/botnet-traffic-analysis/anomaly-detection/logs/events.out.tfevents.1542219160.ATHENA (deflated 88%)\n",
            "  adding: content/botnet-traffic-analysis/anomaly-detection/logs/events.out.tfevents.1542234289.ATHENA (deflated 85%)\n",
            "  adding: content/botnet-traffic-analysis/anomaly-detection/logs/events.out.tfevents.1542232557.ATHENA (deflated 87%)\n",
            "  adding: content/botnet-traffic-analysis/anomaly-detection/logs/events.out.tfevents.1542309448.ATHENA (deflated 85%)\n",
            "  adding: content/botnet-traffic-analysis/anomaly-detection/threshold_115 (stored 0%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0UR5eO2BFt6S",
        "outputId": "5b6f6edb-3fe7-499c-a31b-24caf056d101"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0pU7HNTGHRQ3"
      },
      "source": [
        "!cp '/content/botnet.zip' '/content/drive/MyDrive/botnet.zip'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GzWHgLDMCYlA"
      },
      "source": [
        "## Load and Run - Link : https://drive.google.com/file/d/1CXiyvKTD2EmJP9T8kbhID0lZ618w_qGW/view?usp=sharing - baalu2424"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ItqlYtieCe7Y"
      },
      "source": [
        "!pip install gdown\n",
        "import gdown\n",
        "url = 'https://drive.google.com/file/d/1CXiyvKTD2EmJP9T8kbhID0lZ618w_qGW/view?usp=sharing' #Check ID\n",
        "output = 'botnet.zip'\n",
        "gdown.download(url, output, quiet=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sjWlQTRvDwPE"
      },
      "source": [
        "import os\n",
        "os.chdir('/content/botnet-traffic-analysis')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q2Svst-1EjGF"
      },
      "source": [
        "# RUN THE TEST.PY FILE"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}